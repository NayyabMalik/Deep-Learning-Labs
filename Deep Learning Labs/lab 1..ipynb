{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "26TByuc1_IXW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZeXf3_n_Rq_",
        "outputId": "8d1aa688-f7f9-4c4d-aa6e-475f878e7424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Output:\n",
            " [[0.5       ]\n",
            " [0.19585705]\n",
            " [0.6684618 ]\n",
            " [0.32934351]]\n"
          ]
        }
      ],
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward propagation\n",
        "def forward(X, weights):\n",
        "  z = np.dot(X, weights)  # Linear combination\n",
        "  return sigmoid(z)       # Apply activation function\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "weights = np.random.randn(2, 1)\n",
        "output = forward(X, weights)\n",
        "print(\"Predicted Output:\\n\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdsjkszwCzo5"
      },
      "source": [
        "Compute the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsylTAhu_y2O",
        "outputId": "136f57ae-4107-4445-8e86-337b626c2820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.29624532259304637\n"
          ]
        }
      ],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "  return np.mean((y_true - y_pred) ** 2)\n",
        "# True labels (logical AND function)\n",
        "y_true = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "loss = mean_squared_error(y_true, output)\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ65yq-1DTqs"
      },
      "source": [
        "Backpropogtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iz6CbYwDBZc"
      },
      "outputs": [],
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "# Backpropagation function\n",
        "def backpropagate(X, y_true, y_pred, weights, learning_rate=0.01):\n",
        "# Output layer error\n",
        "  error = y_true - y_pred\n",
        "# Gradient for output layer (using chain rule)\n",
        "  d_weights = np.dot(X.T, error * sigmoid_derivative(y_pred))\n",
        "# Update the weights using the gradients\n",
        "  weights += d_weights * learning_rate\n",
        "  return weights\n",
        "# Perform one step of backpropagation\n",
        "weights = backpropagate(X, y_true, output, weights) ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fazpPMqDdDt",
        "outputId": "682a049c-789c-4719-db1c-4c497ae67ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.70123608]\n",
            " [-1.41121912]]\n"
          ]
        }
      ],
      "source": [
        "print(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DleHBPn6HcPR"
      },
      "source": [
        "train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqD25UxiDhjr",
        "outputId": "2fddfa67-d8e0-4476-e81e-bede1072a67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.250000177300593\n",
            "Epoch 100, Loss: 0.25000015646118856\n",
            "Epoch 200, Loss: 0.2500001380711799\n",
            "Epoch 300, Loss: 0.2500001218426748\n",
            "Epoch 400, Loss: 0.2500001075216184\n",
            "Epoch 500, Loss: 0.25000009488381614\n",
            "Epoch 600, Loss: 0.25000008373142424\n",
            "Epoch 700, Loss: 0.2500000738898525\n",
            "Epoch 800, Loss: 0.25000006520503143\n",
            "Epoch 900, Loss: 0.2500000575410002\n",
            "Epoch 1000, Loss: 0.25000005077777837\n",
            "Epoch 1100, Loss: 0.25000004480948745\n",
            "Epoch 1200, Loss: 0.25000003954269356\n",
            "Epoch 1300, Loss: 0.2500000348949448\n",
            "Epoch 1400, Loss: 0.2500000307934804\n",
            "Epoch 1500, Loss: 0.2500000271740915\n",
            "Epoch 1600, Loss: 0.2500000239801163\n",
            "Epoch 1700, Loss: 0.25000002116155284\n",
            "Epoch 1800, Loss: 0.2500000186742761\n",
            "Epoch 1900, Loss: 0.2500000164793478\n",
            "Epoch 2000, Loss: 0.25000001454240567\n",
            "Epoch 2100, Loss: 0.25000001283312695\n",
            "Epoch 2200, Loss: 0.25000001132475264\n",
            "Epoch 2300, Loss: 0.2500000099936689\n",
            "Epoch 2400, Loss: 0.25000000881903744\n",
            "Epoch 2500, Loss: 0.25000000778246934\n",
            "Epoch 2600, Loss: 0.2500000068677368\n",
            "Epoch 2700, Loss: 0.25000000606051975\n",
            "Epoch 2800, Loss: 0.25000000534818095\n",
            "Epoch 2900, Loss: 0.2500000047195688\n",
            "Epoch 3000, Loss: 0.25000000416484214\n",
            "Epoch 3100, Loss: 0.25000000367531666\n",
            "Epoch 3200, Loss: 0.2500000032433288\n",
            "Epoch 3300, Loss: 0.2500000028621159\n",
            "Epoch 3400, Loss: 0.2500000025257097\n",
            "Epoch 3500, Loss: 0.2500000022288439\n",
            "Epoch 3600, Loss: 0.250000001966871\n",
            "Epoch 3700, Loss: 0.2500000017356897\n",
            "Epoch 3800, Loss: 0.25000000153168095\n",
            "Epoch 3900, Loss: 0.2500000013516509\n",
            "Epoch 4000, Loss: 0.2500000011927811\n",
            "Epoch 4100, Loss: 0.25000000105258446\n",
            "Epoch 4200, Loss: 0.25000000092886626\n",
            "Epoch 4300, Loss: 0.2500000008196896\n",
            "Epoch 4400, Loss: 0.2500000007233453\n",
            "Epoch 4500, Loss: 0.25000000063832506\n",
            "Epoch 4600, Loss: 0.2500000005632978\n",
            "Epoch 4700, Loss: 0.25000000049708915\n",
            "Epoch 4800, Loss: 0.25000000043866244\n",
            "Epoch 4900, Loss: 0.2500000003871031\n",
            "Epoch 5000, Loss: 0.25000000034160397\n",
            "Epoch 5100, Loss: 0.2500000003014527\n",
            "Epoch 5200, Loss: 0.25000000026602065\n",
            "Epoch 5300, Loss: 0.2500000002347532\n",
            "Epoch 5400, Loss: 0.25000000020716096\n",
            "Epoch 5500, Loss: 0.2500000001828117\n",
            "Epoch 5600, Loss: 0.25000000016132445\n",
            "Epoch 5700, Loss: 0.2500000001423628\n",
            "Epoch 5800, Loss: 0.2500000001256298\n",
            "Epoch 5900, Loss: 0.25000000011086354\n",
            "Epoch 6000, Loss: 0.25000000009783296\n",
            "Epoch 6100, Loss: 0.25000000008633394\n",
            "Epoch 6200, Loss: 0.2500000000761864\n",
            "Epoch 6300, Loss: 0.25000000006723166\n",
            "Epoch 6400, Loss: 0.2500000000593294\n",
            "Epoch 6500, Loss: 0.25000000005235595\n",
            "Epoch 6600, Loss: 0.25000000004620215\n",
            "Epoch 6700, Loss: 0.2500000000407717\n",
            "Epoch 6800, Loss: 0.2500000000359795\n",
            "Epoch 6900, Loss: 0.2500000000317505\n",
            "Epoch 7000, Loss: 0.2500000000280187\n",
            "Epoch 7100, Loss: 0.25000000002472544\n",
            "Epoch 7200, Loss: 0.25000000002181927\n",
            "Epoch 7300, Loss: 0.25000000001925465\n",
            "Epoch 7400, Loss: 0.2500000000169915\n",
            "Epoch 7500, Loss: 0.25000000001499434\n",
            "Epoch 7600, Loss: 0.25000000001323197\n",
            "Epoch 7700, Loss: 0.2500000000116767\n",
            "Epoch 7800, Loss: 0.25000000001030426\n",
            "Epoch 7900, Loss: 0.25000000000909306\n",
            "Epoch 8000, Loss: 0.25000000000802436\n",
            "Epoch 8100, Loss: 0.25000000000708117\n",
            "Epoch 8200, Loss: 0.25000000000624883\n",
            "Epoch 8300, Loss: 0.25000000000551437\n",
            "Epoch 8400, Loss: 0.2500000000048662\n",
            "Epoch 8500, Loss: 0.2500000000042943\n",
            "Epoch 8600, Loss: 0.2500000000037895\n",
            "Epoch 8700, Loss: 0.2500000000033441\n",
            "Epoch 8800, Loss: 0.2500000000029511\n",
            "Epoch 8900, Loss: 0.25000000000260414\n",
            "Epoch 9000, Loss: 0.25000000000229816\n",
            "Epoch 9100, Loss: 0.25000000000202804\n",
            "Epoch 9200, Loss: 0.2500000000017896\n",
            "Epoch 9300, Loss: 0.2500000000015793\n",
            "Epoch 9400, Loss: 0.25000000000139366\n",
            "Epoch 9500, Loss: 0.25000000000122985\n",
            "Epoch 9600, Loss: 0.2500000000010853\n",
            "Epoch 9700, Loss: 0.25000000000095773\n",
            "Epoch 9800, Loss: 0.2500000000008451\n",
            "Epoch 9900, Loss: 0.25000000000074585\n"
          ]
        }
      ],
      "source": [
        "def train(X, y_true, weights, epochs=10000, learning_rate=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        y_pred = forward(X, weights)\n",
        "\n",
        "        # Backpropagation and weight update\n",
        "        weights = backpropagate(X, y_true, y_pred, weights, learning_rate)\n",
        "\n",
        "        # Print loss every 1000 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            loss = mean_squared_error(y_true, y_pred)\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Train the network\n",
        "weights = train(X, y_true, weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4b_WzgVHiB-",
        "outputId": "3bc89fa6-1c86-4901-d98c-1db1624d27c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Predicted Output:\n",
            " [[0.5       ]\n",
            " [0.49999885]\n",
            " [0.50000115]\n",
            " [0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test the trained network\n",
        "final_output = forward(X, weights)\n",
        "print(\"Final Predicted Output:\\n\", final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "92hQWeqcIQBW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "experiement"
      ],
      "metadata": {
        "id": "UmRVh6FAsHnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error (MSE) Loss function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Forward propagation\n",
        "def forward(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Backpropagation function\n",
        "def backpropagate(X, y_true, y_pred, weights, bias, learning_rate=0.01):\n",
        "    # Output layer error\n",
        "    error = y_true - y_pred\n",
        "\n",
        "    # Gradient for weights and bias (using chain rule)\n",
        "    d_weights = np.dot(X.T, error * sigmoid_derivative(y_pred))\n",
        "    d_bias = np.sum(error * sigmoid_derivative(y_pred), axis=0, keepdims=True)\n",
        "\n",
        "    # Update the weights and bias using the gradients\n",
        "    weights += d_weights * learning_rate\n",
        "    bias += d_bias * learning_rate\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Training function\n",
        "def train(X, y_true, weights, bias, epochs=10000, learning_rate=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        y_pred = forward(X, weights, bias)\n",
        "\n",
        "        # Backpropagation and weight update\n",
        "        weights, bias = backpropagate(X, y_true, y_pred, weights, bias, learning_rate)\n",
        "\n",
        "        # Print loss every 1000 epochs\n",
        "        if epoch % 1000 == 0:\n",
        "            loss = mean_squared_error(y_true, y_pred)\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Input data (X) and true output (y_true)\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Logical AND inputs\n",
        "    y_true = np.array([[0], [0], [0], [1]])         # Logical AND outputs\n",
        "\n",
        "    # Initialize weights and bias\n",
        "    weights = np.random.randn(2, 1)  # Random weights (2 inputs to 1 output)\n",
        "    bias = np.random.randn(1, 1)     # Random bias\n",
        "\n",
        "    print(\"Initial Weights:\\n\", weights)\n",
        "    print(\"Initial Bias:\\n\", bias)\n",
        "\n",
        "    # Train the network\n",
        "    trained_weights, trained_bias = train(X, y_true, weights, bias)\n",
        "\n",
        "    # Test the trained network\n",
        "    final_output = forward(X, trained_weights, trained_bias)\n",
        "    print(\"Final Predicted Output:\\n\", final_output)\n",
        "\n",
        "    # Experiment with hyperparameters\n",
        "    print(\"\\nExperimenting with learning rate:\")\n",
        "    trained_weights_lr, trained_bias_lr = train(X, y_true, weights, bias, learning_rate=0.1)\n",
        "    final_output_lr = forward(X, trained_weights_lr, trained_bias_lr)\n",
        "    print(\"Final Predicted Output with higher learning rate:\\n\", final_output_lr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVGkkSOYrW8p",
        "outputId": "ec798bd1-6392-4383-beaf-888d9e3fec6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            " [[ 1.02659149]\n",
            " [-0.66995925]]\n",
            "Initial Bias:\n",
            " [[0.39375643]]\n",
            "Epoch 0, Loss: 0.32356366615928667\n",
            "Epoch 1000, Loss: 0.1559169530920663\n",
            "Epoch 2000, Loss: 0.10982602248630877\n",
            "Epoch 3000, Loss: 0.08565490491815281\n",
            "Epoch 4000, Loss: 0.07042669701248712\n",
            "Epoch 5000, Loss: 0.059679142988282446\n",
            "Epoch 6000, Loss: 0.05159787206945214\n",
            "Epoch 7000, Loss: 0.0452825740592855\n",
            "Epoch 8000, Loss: 0.040215417140710294\n",
            "Epoch 9000, Loss: 0.036068359799585586\n",
            "Final Predicted Output:\n",
            " [[0.01737299]\n",
            " [0.19286138]\n",
            " [0.19360847]\n",
            " [0.76441832]]\n",
            "\n",
            "Experimenting with learning rate:\n",
            "Epoch 0, Loss: 0.032620075416857366\n",
            "Epoch 1000, Loss: 0.015893651278124275\n",
            "Epoch 2000, Loss: 0.010152368642912104\n",
            "Epoch 3000, Loss: 0.0073562499463891585\n",
            "Epoch 4000, Loss: 0.005727879842854609\n",
            "Epoch 5000, Loss: 0.00467097253553545\n",
            "Epoch 6000, Loss: 0.003933300478012456\n",
            "Epoch 7000, Loss: 0.0033909682903290415\n",
            "Epoch 8000, Loss: 0.0029763888167747823\n",
            "Epoch 9000, Loss: 0.0026497093166353344\n",
            "Final Predicted Output with higher learning rate:\n",
            " [[2.08999168e-04]\n",
            " [5.28595772e-02]\n",
            " [5.28595782e-02]\n",
            " [9.37106516e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HVdNHuFsJvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}